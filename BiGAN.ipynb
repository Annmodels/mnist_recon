{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BiGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN6dVXgu+dojNnDgMX2o7zJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Annmodels/mnist_recon/blob/master/BiGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FG-W7hsvU092",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qAYEc60UK1c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
        "!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
        "!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
        "!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1LN8GzXWrjy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir MNIST_Fashion\n",
        "!mv *.gz MNIST_Fashion/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bonyblRXlSL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets('MNIST_Fashion/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fr-y2YaGYjyz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize = (1,1))\n",
        "sample_image = mnist.train.next_batch(1)[0]\n",
        "sample_image = sample_image.reshape([28,28])\n",
        "plt.imshow(sample_image , cmap = \"gray\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXAK768jeTZB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lrate = 0.0002\n",
        "batch_size = 100\n",
        "epochs = 120000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpDj-J-rewDQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_dim = 784\n",
        "z_dim = 64\n",
        "en_h1 = 256\n",
        "d_h1 = 256\n",
        "disc_h1 = 256"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jI6vmNH_fPTN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def xavier_init(shape):\n",
        "  return tf.random_normal(shape = shape , stddev = 1./tf.sqrt(shape[0]/2.0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZOGtiQEf5WD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "en_wt = {\n",
        "    \"en_h1\": tf.Variable(xavier_init([image_dim , en_h1])),\n",
        "    \"en_final\": tf.Variable(xavier_init([en_h1 , z_dim]))\n",
        "        }\n",
        "d_wt = {\n",
        "    \"d_h1\": tf.Variable(xavier_init([z_dim , d_h1])),\n",
        "    \"d_final\": tf.Variable(xavier_init([d_h1 , image_dim]))\n",
        "       }\n",
        "disc_wt = {\n",
        "    \"disc_h1\": tf.Variable(xavier_init([image_dim+z_dim , disc_h1])),\n",
        "    \"disc_final\": tf.Variable(xavier_init([disc_h1 , 1])),\n",
        "          }\n",
        "\n",
        "en_bias = {\n",
        "    \"en_h1\": tf.Variable(xavier_init([en_h1])),\n",
        "    \"en_final\": tf.Variable(xavier_init([z_dim]))\n",
        "          }\n",
        "d_bias = {\n",
        "    \"d_h1\": tf.Variable(xavier_init([d_h1])),\n",
        "    \"d_final\": tf.Variable(xavier_init([image_dim]))\n",
        "          }\n",
        "disc_bias = {\n",
        "    \"disc_h1\": tf.Variable(xavier_init([disc_h1])),\n",
        "    \"disc_final\": tf.Variable(xavier_init([1]))\n",
        "           }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pwW1nOdyMWt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z_input = tf.placeholder(tf.float32 , shape = [None , z_dim])\n",
        "x_input = tf.placeholder(tf.float32 , shape = [None , image_dim])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kITDPv0vgM0L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def z_distribution(m,n):\n",
        "  return np.random.uniform(-1.,1.,size = [m,n])\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqFQ5sicvAPC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Discriminator(x,z):\n",
        "  input = tf.concat(axis = 1 , values = [x,z])\n",
        "  disc_hidn_layer = tf.nn.relu(tf.add(tf.matmul(input , disc_wt[\"disc_h1\"]) , disc_bias[\"disc_h1\"]))\n",
        "  final_layer = tf.add(tf.matmul(disc_hidn_layer , disc_wt[\"disc_final\"]) , disc_bias[\"disc_final\"])\n",
        "  disc_prob_output = tf.nn.sigmoid(final_layer)\n",
        "  return disc_prob_output\n",
        "\n",
        "def Encoder(x):\n",
        "  en_hidn_layer = tf.nn.relu(tf.add(tf.matmul(x , en_wt[\"en_h1\"]) , en_bias[\"en_h1\"]))\n",
        "  final_layer = tf.add(tf.matmul(en_hidn_layer , en_wt[\"en_final\"]) , en_bias[\"en_final\"])\n",
        "  en_output = final_layer\n",
        "  return en_output\n",
        "\n",
        "def Decoder(z):\n",
        "  d_hidn_layer = tf.nn.relu(tf.add(tf.matmul(z , d_wt[\"d_h1\"]) , d_bias[\"d_h1\"]))\n",
        "  final_layer = tf.add(tf.matmul(d_hidn_layer , d_wt[\"d_final\"]) , d_bias[\"d_final\"])\n",
        "  gen_prob_output = tf.nn.sigmoid(final_layer)\n",
        "  return gen_prob_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kKRFcExrP4O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z_generated = Encoder(x_input)\n",
        "x_generated = Decoder(z_input)\n",
        "real_output_disc = Discriminator(x_input,z_generated)\n",
        "fake_output_disc = Discriminator(x_generated,z_input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9Ld2qWlpmMx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Discriminator_Loss = -tf.reduce_mean(tf.log(real_output_disc + 1e-7)+tf.log(1.0-fake_output_disc + 1e-7))\n",
        "Decoder_Loss = -tf.reduce_mean(tf.log(fake_output_disc + 1e-7)+tf.log(1.0-real_output_disc + 1e-7))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKo-KWM-3eAo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Encoder_var = [en_wt[\"en_h1\"],en_wt[\"en_final\"],en_bias[\"en_h1\"],en_bias[\"en_final\"]]\n",
        "Decoder_var = [d_wt[\"d_h1\"],d_wt[\"d_final\"],d_bias[\"d_h1\"],d_bias[\"d_final\"]]\n",
        "Discriminator_var = [disc_wt[\"disc_h1\"],disc_wt[\"disc_final\"],disc_bias[\"disc_h1\"],disc_bias[\"disc_final\"]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OH3jmZwP6NDu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.name_scope(\"Optimizer_Discriminator\")as scope:\n",
        "  Discriminator_optimize = tf.train.AdamOptimizer(learning_rate = lrate).minimize(Discriminator_Loss , var_list = Discriminator_var)\n",
        "\n",
        "with tf.name_scope(\"Optimizer_Decoder\")as scope:\n",
        "  Decoder_optimize = tf.train.AdamOptimizer(learning_rate = lrate).minimize(Decoder_Loss , var_list = Encoder_var+Decoder_var)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9L2xcQgm8Cty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "init = tf.global_variables_initializer()\n",
        "sess = tf.Session()\n",
        "sess.run(init)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  x_batch,_ = mnist.train.next_batch(batch_size)\n",
        "  z_noise = z_distribution(batch_size,z_dim)\n",
        "  #np.random.uniform(-1.,1.,size = [batch_size , z_noise_dim]) \n",
        "  _,Disc_loss_epoch = sess.run([Discriminator_optimize,Discriminator_Loss],feed_dict = {x_input:x_batch,z_input:z_noise}) \n",
        "  _,D_loss_epoch = sess.run([Decoder_optimize,Decoder_Loss],feed_dict = {x_input:x_batch,z_input:z_noise})\n",
        "  if epoch%2000 == 0:\n",
        "    print(\"steps: {0}  D_loss: {1} , Disc_loss: {2}\".format(epoch,D_loss_epoch,Disc_loss_epoch))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBULlzhBGV3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_output = Decoder(z_input)\n",
        "n = 6\n",
        "canvas = np.empty((28*n,28*n))\n",
        "for i in range(n):\n",
        "  z_noise = z_distribution(batch_size,z_dim)\n",
        "  #np.random.uniform(-1.,1.,size = [batch_size , z_noise_dim])\n",
        "  g = sess.run(test_output , feed_dict = {z_input:z_noise})\n",
        "  g = -1*(g-1)\n",
        "  for j in range(n):\n",
        "    canvas[i*28:(i+1)*28 , j*28:(j+1)*28] = g[j].reshape([28,28])\n",
        "\n",
        "plt.figure(figsize = (n,n))\n",
        "plt.imshow(canvas , origin = \"upper\" , cmap = \"gray\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKric6pnT7w5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z_generated = Encoder(x_input)\n",
        "test_output = Decoder(z_generated)\n",
        "n = 6\n",
        "canvas1 = np.empty((28*n , 28*n))\n",
        "canvas2 = np.empty((28*n , 28*n))\n",
        "\n",
        "for i in range(n):\n",
        "  x_batch,_ = mnist.train.next_batch(batch_size)\n",
        "  g_x = sess.run(test_output,feed_dict = {x_input:x_batch})\n",
        "  #print(g.shape)\n",
        "  for j in range(n):\n",
        "    canvas1[i*28:(i+1)*28,j*28:(j+1)*28] = g_x[j].reshape([28,28])\n",
        "  for j in range(n):\n",
        "    canvas2[i*28:(i+1)*28,j*28:(j+1)*28] = x_batch[j].reshape([28,28])\n",
        "\n",
        "f,ax = plt.subplots(1,2)\n",
        "ax[0].imshow(canvas1 , origin = \"upper\" , cmap = \"gray\")\n",
        "ax[1].imshow(canvas2 , origin = \"upper\" , cmap = \"gray\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}